{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Anaconda and xgboost\n",
    "\n",
    "## original post: http://www.kdnuggets.com/2017/03/simple-xgboost-tutorial-iris-dataset.html\n",
    "\n",
    "In order to work with the data, I need to install various scientific libraries for python. The best way I have found is to use Anaconda. It simply installs all the libs and helps to install new ones. You can download the installer for Windows, but if you want to install it on a Linux server, you can just copy-paste this into the terminal:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wget http://repo.continuum.io/archive/Anaconda2-4.0.0-Linux-x86_64.sh\n",
    "bash Anaconda2-4.0.0-Linux-x86_64.sh -b -p $HOME/anaconda\n",
    "echo 'export PATH=\"$HOME/anaconda/bin:$PATH\"' >> ~/.bashrc\n",
    "bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, use conda to install pip which you will need for installing xgboost. It is important to install it using Anaconda (in Anaconda’s directory), so that pip installs other libs there as well:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conda install -y pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a very important step: install xgboost Python Package dependencies beforehand. I install these ones from experience:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sudo apt-get install -y make g++ build-essential gfortran libatlas-base-dev liblapacke-dev python-dev python-setuptools libsm6 libxrender1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I upgrade my python virtual environment to have no trouble with python versions:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install --upgrade virtualenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally I can install xgboost with pip (keep fingers crossed):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command installs the latest xgboost version, but if you want to use a previous one, just specify it with:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install xgboost==0.4a30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test if everything is has gone well – type python in the terminal and try to import xgboost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see no errors – perfect.\n",
    "\n",
    "# Xgboost Demo with the Iris Dataset\n",
    " \n",
    "Here I will use the Iris dataset to show a simple example of how to use Xgboost.\n",
    "\n",
    "First you load the dataset from sklearn, where X will be the data, y – the class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you split the data into train and test sets with 80-20% split (http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to create the Xgboost specific DMatrix (http://xgboost.readthedocs.io/en/latest/python/python_intro.html) data format from the numpy array. Xgboost can work with numpy arrays directly, load data from svmlignt files and other formats. Here is how to work with numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use svmlight for less memory consumption, first dump (http://scikit-learn.org/stable/modules/generated/sklearn.datasets.dump_svmlight_file.html) the numpy array into svmlight format and then just pass the filename to DMatrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "dump_svmlight_file(X_train, y_train, 'dtrain.svm', zero_based=True)\n",
    "dump_svmlight_file(X_test, y_test, 'dtest.svm', zero_based=True)\n",
    "dtrain_svm = xgb.DMatrix('dtrain.svm')\n",
    "dtest_svm = xgb.DMatrix('dtest.svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the Xgboost to work you need to set the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 3,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 3}  # the number of classes that exist in this datset\n",
    "num_round = 20  # the number of training iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different datasets perform better with different parameters. The result can be really low with one set of params and really good with others. You can look at this Kaggle script (https://www.kaggle.com/tanitter/introducing-kaggle-scripts/grid-search-xgboost-with-scikit-learn) how to search for the best ones. Generally try with eta 0.1, 0.2, 0.3, max_depth in range of 2 to 10 and num_round around few hundred.\n",
    "\n",
    "# Train\n",
    " \n",
    "Finally the training can begin. You just type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the model looks you can also dump (http://xgboost.readthedocs.io/en/latest/python/python_intro.html#training) it in human readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst.dump_model('dump.raw.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it looks something like this (f0, f1, f2 are features):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "booster[0]:\n",
    "0:[f2<2.45] yes=1,no=2,missing=1\n",
    "    1:leaf=0.426036\n",
    "    2:leaf=-0.218845\n",
    "booster[1]:\n",
    "0:[f2<2.45] yes=1,no=2,missing=1\n",
    "    1:leaf=-0.213018\n",
    "    2:[f3<1.75] yes=3,no=4,missing=3\n",
    "        3:[f2<4.95] yes=5,no=6,missing=5\n",
    "            5:leaf=0.409091\n",
    "            6:leaf=-9.75349e-009\n",
    "        4:[f2<4.85] yes=7,no=8,missing=7\n",
    "            7:leaf=-7.66345e-009\n",
    "            8:leaf=-0.210219\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that each tree is no deeper than 3 levels as set in the params.\n",
    "\n",
    "Use the model to predict classes (http://xgboost.readthedocs.io/en/latest/python/python_intro.html#prediction) for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the predictions look something like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[ 0.00563804 0.97755206 0.01680986]\n",
    " [ 0.98254657 0.01395847 0.00349498]\n",
    " [ 0.0036375 0.00615226 0.99021029]\n",
    " [ 0.00564738 0.97917044 0.0151822 ]\n",
    " [ 0.00540075 0.93640935 0.0581899 ]\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here each column represents class number 0, 1, or 2. For each line you need to select that column where the probability is the highest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you get a nice list with predicted classes:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[1, 0, 2, 1, 1, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the precision (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) of this prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print precision_score(y_test, best_preds, average='macro')\n",
    "# >> 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now save (http://scikit-learn.org/stable/modules/model_persistence.html) the model for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bst_model.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(bst, 'bst_model.pkl', compress=True)\n",
    "# bst = joblib.load('bst_model.pkl') # load it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a working model saved for later use, and ready for more prediction.\n",
    "\n",
    "See the full code on github (https://gist.github.com/IevaZarina/ef63197e089169a9ea9f3109058a9679) or below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Ieva Zarina, 2016, licensed under the Apache 2.0 licnese\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# use DMatrix for xgbosot\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# use svmlight file for xgboost\n",
    "dump_svmlight_file(X_train, y_train, 'dtrain.svm', zero_based=True)\n",
    "dump_svmlight_file(X_test, y_test, 'dtest.svm', zero_based=True)\n",
    "dtrain_svm = xgb.DMatrix('dtrain.svm')\n",
    "dtest_svm = xgb.DMatrix('dtest.svm')\n",
    "\n",
    "# set xgboost params\n",
    "param = {\n",
    "    'max_depth': 3,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 3}  # the number of classes that exist in this datset\n",
    "num_round = 20  # the number of training iterations\n",
    "\n",
    "#------------- numpy array ------------------\n",
    "# training and testing - numpy matrices\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "# extracting most confident predictions\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "print \"Numpy array precision:\", precision_score(y_test, best_preds, average='macro')\n",
    "\n",
    "# ------------- svm file ---------------------\n",
    "# training and testing - svm file\n",
    "bst_svm = xgb.train(param, dtrain_svm, num_round)\n",
    "preds = bst.predict(dtest_svm)\n",
    "\n",
    "# extracting most confident predictions\n",
    "best_preds_svm = [np.argmax(line) for line in preds]\n",
    "print \"Svm file precision:\",precision_score(y_test, best_preds_svm, average='macro')\n",
    "# --------------------------------------------\n",
    "\n",
    "# dump the models\n",
    "bst.dump_model('dump.raw.txt')\n",
    "bst_svm.dump_model('dump_svm.raw.txt')\n",
    "\n",
    "\n",
    "# save the models for later\n",
    "joblib.dump(bst, 'bst_model.pkl', compress=True)\n",
    "joblib.dump(bst_svm, 'bst_svm_model.pkl', compress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
